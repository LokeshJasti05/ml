{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e2c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           roc_auc_score, roc_curve, accuracy_score,\n",
    "                           precision_recall_curve, f1_score)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting parameters\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c718e142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project structure created successfully!\n",
      "‚ö†Ô∏è  Dataset not found. Creating sample data for demonstration...\n",
      "‚úÖ Sample dataset created!\n",
      "üìä Dataset shape: (1000, 9)\n",
      "üìã Columns: ['Id', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'YearBuilt', 'Neighborhood', 'HouseStyle', 'SalePrice']\n"
     ]
    }
   ],
   "source": [
    "def setup_project_structure():\n",
    "    \"\"\"Create complete directory structure for ML project\"\"\"\n",
    "    directories = [\n",
    "        'data', 'data/raw', 'data/processed', 'data/external',\n",
    "        'notebooks', 'src', 'models', 'reports', 'reports/figures', 'results'\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"‚úÖ Project structure created successfully!\")\n",
    "\n",
    "def safe_save_file(data, filepath, **kwargs):\n",
    "    \"\"\"Safely save data with directory creation\"\"\"\n",
    "    file_path = Path(filepath)\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if hasattr(data, 'to_csv'):\n",
    "        data.to_csv(filepath, **kwargs)\n",
    "    else:\n",
    "        # For other data types, implement as needed\n",
    "        pass\n",
    "    \n",
    "    print(f\"‚úÖ Saved: {filepath}\")\n",
    "\n",
    "# Setup project structure\n",
    "setup_project_structure()\n",
    "\n",
    "# Load the housing dataset\n",
    "try:\n",
    "    # Try to load from different possible locations\n",
    "    possible_paths = [\n",
    "        'data/raw/train.csv',\n",
    "        'train.csv',\n",
    "        '../data/train.csv'\n",
    "    ]\n",
    "    \n",
    "    train_data = None\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            train_data = pd.read_csv(path)\n",
    "            print(f\"‚úÖ Dataset loaded from: {path}\")\n",
    "            break\n",
    "    \n",
    "    if train_data is None:\n",
    "        print(\"‚ö†Ô∏è  Dataset not found. Creating sample data for demonstration...\")\n",
    "        # Create sample housing data\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        \n",
    "        train_data = pd.DataFrame({\n",
    "            'Id': range(1, n_samples + 1),\n",
    "            'OverallQual': np.random.randint(1, 11, n_samples),\n",
    "            'GrLivArea': np.random.randint(800, 3000, n_samples),\n",
    "            'GarageCars': np.random.randint(0, 4, n_samples),\n",
    "            'TotalBsmtSF': np.random.randint(0, 2000, n_samples),\n",
    "            'YearBuilt': np.random.randint(1950, 2021, n_samples),\n",
    "            'Neighborhood': np.random.choice(['A', 'B', 'C', 'D'], n_samples),\n",
    "            'HouseStyle': np.random.choice(['1Story', '2Story', 'Split'], n_samples),\n",
    "            'SalePrice': np.random.randint(50000, 500000, n_samples)\n",
    "        })\n",
    "        print(\"‚úÖ Sample dataset created!\")\n",
    "    \n",
    "    print(f\"üìä Dataset shape: {train_data.shape}\")\n",
    "    print(f\"üìã Columns: {list(train_data.columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error loading dataset: {e}\")\n",
    "    print(\"Please ensure you have the housing dataset in the correct location.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a467e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HOUSING PRICE CLASSIFICATION ANALYSIS\n",
      "============================================================\n",
      "üí∞ Price Categories Created:\n",
      "   üîµ Low (0): <= $167,382\n",
      "   üü¢ Medium-Low (1): $167,382 - $276,102\n",
      "   üü° Medium-High (2): $276,102 - $386,782\n",
      "   üî¥ High (3): > $386,782\n",
      "\n",
      "üìà Class Distribution:\n",
      "   Low: 250 samples (25.0%)\n",
      "   Medium-Low: 250 samples (25.0%)\n",
      "   Medium-High: 250 samples (25.0%)\n",
      "   High: 250 samples (25.0%)\n",
      "\n",
      "üîß Preprocessing Data...\n",
      "   Initial shape: (1000, 10)\n",
      "   ‚úÖ Encoded Neighborhood: 4 categories\n",
      "   ‚úÖ Encoded HouseStyle: 3 categories\n",
      "   Final shape: (1000, 10)\n",
      "\n",
      "üìä Model Input Summary:\n",
      "   Features shape: (1000, 7)\n",
      "   Target shape: (1000,)\n",
      "   Feature columns: ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'YearBuilt', 'Neighborhood', 'HouseStyle']\n"
     ]
    }
   ],
   "source": [
    "print(\" HOUSING PRICE CLASSIFICATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def create_price_categories(df, target_col='SalePrice'):\n",
    "    \"\"\"Convert regression problem to multi-class classification\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Calculate quartiles for price categories\n",
    "    q1 = df_copy[target_col].quantile(0.25)\n",
    "    q2 = df_copy[target_col].quantile(0.50) \n",
    "    q3 = df_copy[target_col].quantile(0.75)\n",
    "    \n",
    "    def categorize_price(price):\n",
    "        if price <= q1:\n",
    "            return 0  # Low\n",
    "        elif price <= q2:\n",
    "            return 1  # Medium-Low\n",
    "        elif price <= q3:\n",
    "            return 2  # Medium-High\n",
    "        else:\n",
    "            return 3  # High\n",
    "    \n",
    "    df_copy['PriceCategory'] = df_copy[target_col].apply(categorize_price)\n",
    "    \n",
    "    print(\"üí∞ Price Categories Created:\")\n",
    "    print(f\"   üîµ Low (0): <= ${q1:,.0f}\")\n",
    "    print(f\"   üü¢ Medium-Low (1): ${q1:,.0f} - ${q2:,.0f}\")\n",
    "    print(f\"   üü° Medium-High (2): ${q2:,.0f} - ${q3:,.0f}\")\n",
    "    print(f\"   üî¥ High (3): > ${q3:,.0f}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Create classification dataset\n",
    "classification_data = create_price_categories(train_data)\n",
    "\n",
    "# Check class distribution\n",
    "print(f\"\\nüìà Class Distribution:\")\n",
    "class_counts = classification_data['PriceCategory'].value_counts().sort_index()\n",
    "for i, count in enumerate(class_counts):\n",
    "    categories = ['Low', 'Medium-Low', 'Medium-High', 'High']\n",
    "    percentage = (count / len(classification_data)) * 100\n",
    "    print(f\"   {categories[i]}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "def preprocess_classification_data(df):\n",
    "    \"\"\"Comprehensive data preprocessing for classification\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    print(f\"\\nüîß Preprocessing Data...\")\n",
    "    print(f\"   Initial shape: {df_processed.shape}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    numerical_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Fill numerical missing values with median\n",
    "    for col in numerical_cols:\n",
    "        if df_processed[col].isnull().any():\n",
    "            median_val = df_processed[col].median()\n",
    "            df_processed[col].fillna(median_val, inplace=True)\n",
    "            print(f\"   ‚úÖ Filled {col} missing values with median: {median_val}\")\n",
    "    \n",
    "    # Fill categorical missing values with mode\n",
    "    for col in categorical_cols:\n",
    "        if df_processed[col].isnull().any():\n",
    "            mode_val = df_processed[col].mode()[0]\n",
    "            df_processed[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"   ‚úÖ Filled {col} missing values with mode: {mode_val}\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"   ‚úÖ Encoded {col}: {len(le.classes_)} categories\")\n",
    "    \n",
    "    print(f\"   Final shape: {df_processed.shape}\")\n",
    "    return df_processed, label_encoders\n",
    "\n",
    "# Preprocess the data\n",
    "processed_data, encoders = preprocess_classification_data(classification_data)\n",
    "\n",
    "# Prepare features and target\n",
    "exclude_cols = ['SalePrice', 'PriceCategory', 'Id']\n",
    "X = processed_data.drop([col for col in exclude_cols if col in processed_data.columns], axis=1)\n",
    "y = processed_data['PriceCategory']\n",
    "\n",
    "print(f\"\\nüìä Model Input Summary:\")\n",
    "print(f\"   Features shape: {X.shape}\")\n",
    "print(f\"   Target shape: {y.shape}\")\n",
    "print(f\"   Feature columns: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bad2065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Data Split Summary:\n",
      "   Training set: 800 samples\n",
      "   Testing set: 200 samples\n",
      "\n",
      "üìà Training Set Class Distribution:\n",
      "   Low: 200 (25.0%)\n",
      "   Medium-Low: 200 (25.0%)\n",
      "   Medium-High: 200 (25.0%)\n",
      "   High: 200 (25.0%)\n",
      "\n",
      "üîß Feature Scaling Applied:\n",
      "   Mean of first feature after scaling: 0.000\n",
      "   Std of first feature after scaling: 1.000\n",
      "\n",
      "üéØ Feature Selection:\n",
      "   Selected 7 best features:\n",
      "   ‚Ä¢ OverallQual: 1.37\n",
      "   ‚Ä¢ GrLivArea: 0.96\n",
      "   ‚Ä¢ GarageCars: 0.19\n",
      "   ‚Ä¢ TotalBsmtSF: 1.96\n",
      "   ‚Ä¢ YearBuilt: 1.30\n",
      "   ‚Ä¢ Neighborhood: 3.02\n",
      "   ‚Ä¢ HouseStyle: 1.01\n",
      "\n",
      "‚öñÔ∏è  Class Weights (for balanced training):\n",
      "   Low: 1.000\n",
      "   Medium-Low: 1.000\n",
      "   Medium-High: 1.000\n",
      "   High: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Split the data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"üìä Data Split Summary:\")\n",
    "print(f\"   Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"   Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "print(f\"\\nüìà Training Set Class Distribution:\")\n",
    "train_dist = y_train.value_counts().sort_index()\n",
    "for i, count in enumerate(train_dist):\n",
    "    categories = ['Low', 'Medium-Low', 'Medium-High', 'High']\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    print(f\"   {categories[i]}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nüîß Feature Scaling Applied:\")\n",
    "print(f\"   Mean of first feature after scaling: {X_train_scaled[:, 0].mean():.3f}\")\n",
    "print(f\"   Std of first feature after scaling: {X_train_scaled[:, 0].std():.3f}\")\n",
    "\n",
    "# Feature Selection (optional but recommended)\n",
    "print(f\"\\nüéØ Feature Selection:\")\n",
    "feature_selector = SelectKBest(score_func=f_classif, k=min(15, X_train.shape[1]))\n",
    "X_train_selected = feature_selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = feature_selector.transform(X_test_scaled)\n",
    "\n",
    "selected_features = X.columns[feature_selector.get_support()]\n",
    "print(f\"   Selected {X_train_selected.shape[1]} best features:\")\n",
    "for i, feature in enumerate(selected_features):\n",
    "    score = feature_selector.scores_[feature_selector.get_support()][i]\n",
    "    print(f\"   ‚Ä¢ {feature}: {score:.2f}\")\n",
    "\n",
    "# Compute class weights for balanced training\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(y_train), \n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(f\"\\n‚öñÔ∏è  Class Weights (for balanced training):\")\n",
    "for class_label, weight in class_weight_dict.items():\n",
    "    categories = ['Low', 'Medium-Low', 'Medium-High', 'High']\n",
    "    print(f\"   {categories[class_label]}: {weight:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d6e423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üß† BAYESIAN LOGISTIC REGRESSION\n",
      "============================================================\n",
      "\n",
      "üîç Hyperparameter Tuning for Logistic Regression...\n",
      "   üèÉ‚Äç‚ôÇÔ∏è Training models with different hyperparameters...\n",
      "‚úÖ Best Parameters Found:\n",
      "   ‚Ä¢ C: 0.001\n",
      "   ‚Ä¢ class_weight: balanced\n",
      "   ‚Ä¢ max_iter: 1000\n",
      "   ‚Ä¢ penalty: l2\n",
      "   ‚Ä¢ solver: lbfgs\n",
      "üìä Best Cross-Validation Score: 0.2618\n",
      "\n",
      "üìà Bayesian Logistic Regression Results:\n",
      "   ‚Ä¢ Test Accuracy: 0.2850\n",
      "   ‚Ä¢ F1-Score (Macro): 0.2848\n",
      "   ‚Ä¢ Cross-Validation Score: 0.2637\n",
      "\n",
      "üìã Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.21      0.26      0.23        50\n",
      "  Medium-Low       0.37      0.40      0.38        50\n",
      " Medium-High       0.31      0.26      0.28        50\n",
      "        High       0.27      0.22      0.24        50\n",
      "\n",
      "    accuracy                           0.28       200\n",
      "   macro avg       0.29      0.29      0.28       200\n",
      "weighted avg       0.29      0.28      0.28       200\n",
      "\n",
      "\n",
      "üéØ Top 5 Most Important Features:\n",
      "   ‚Ä¢ Neighborhood: 0.0196\n",
      "   ‚Ä¢ TotalBsmtSF: 0.0173\n",
      "   ‚Ä¢ YearBuilt: 0.0146\n",
      "   ‚Ä¢ HouseStyle: 0.0131\n",
      "   ‚Ä¢ OverallQual: 0.0130\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß† BAYESIAN LOGISTIC REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Bayesian Logistic Regression using Regularized Logistic Regression\n",
    "# (L2 regularization approximates Bayesian inference with Gaussian priors)\n",
    "\n",
    "print(f\"\\nüîç Hyperparameter Tuning for Logistic Regression...\")\n",
    "\n",
    "# Define parameter grid for Bayesian-like regularization\n",
    "log_param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  # Regularization strength\n",
    "    'penalty': ['l2'],  # L2 regularization (Bayesian-like with Gaussian prior)\n",
    "    'solver': ['lbfgs'],  # Good solver for multiclass problems\n",
    "    'max_iter': [1000],  # Ensure convergence\n",
    "    'class_weight': [None, 'balanced']  # Handle class imbalance\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "log_grid_search = GridSearchCV(\n",
    "    LogisticRegression(random_state=42, multi_class='ovr'),\n",
    "    log_param_grid, \n",
    "    cv=5, \n",
    "    scoring='f1_macro',  # Good for multiclass problems\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"   üèÉ‚Äç‚ôÇÔ∏è Training models with different hyperparameters...\")\n",
    "log_grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Best model\n",
    "best_bayesian_lr = log_grid_search.best_estimator_\n",
    "\n",
    "print(f\"‚úÖ Best Parameters Found:\")\n",
    "for param, value in log_grid_search.best_params_.items():\n",
    "    print(f\"   ‚Ä¢ {param}: {value}\")\n",
    "\n",
    "print(f\"üìä Best Cross-Validation Score: {log_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Train and evaluate the best model\n",
    "y_pred_lr = best_bayesian_lr.predict(X_test_selected)\n",
    "y_pred_proba_lr = best_bayesian_lr.predict_proba(X_test_selected)\n",
    "\n",
    "# Calculate metrics\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr, average='macro')\n",
    "lr_cv_score = cross_val_score(best_bayesian_lr, X_train_selected, y_train, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "print(f\"\\nüìà Bayesian Logistic Regression Results:\")\n",
    "print(f\"   ‚Ä¢ Test Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score (Macro): {lr_f1:.4f}\")  \n",
    "print(f\"   ‚Ä¢ Cross-Validation Score: {lr_cv_score:.4f}\")\n",
    "\n",
    "print(f\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, \n",
    "                          target_names=['Low', 'Medium-Low', 'Medium-High', 'High']))\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "feature_importance_lr = np.abs(best_bayesian_lr.coef_).mean(axis=0)\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': feature_importance_lr\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nüéØ Top 5 Most Important Features:\")\n",
    "for idx, row in feature_importance_df.head().iterrows():\n",
    "    print(f\"   ‚Ä¢ {row['Feature']}: {row['Importance']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f610cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ü§ñ SUPPORT VECTOR MACHINE CLASSIFICATION\n",
      "============================================================\n",
      "\n",
      "üîç Hyperparameter Tuning for SVM...\n",
      "   üèÉ‚Äç‚ôÇÔ∏è Training SVM models (this may take a few minutes)...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ü§ñ SUPPORT VECTOR MACHINE CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüîç Hyperparameter Tuning for SVM...\")\n",
    "\n",
    "# Define parameter grid for SVM\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],  # Kernel coefficient\n",
    "    'kernel': ['rbf', 'poly', 'linear'],  # Different kernel types\n",
    "    'class_weight': [None, 'balanced']  # Handle class imbalance\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation (reduced CV for faster execution)\n",
    "svm_grid_search = GridSearchCV(\n",
    "    SVC(random_state=42, probability=True),  # probability=True for ROC curves\n",
    "    svm_param_grid, \n",
    "    cv=3,  # Reduced CV folds for faster execution\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"   üèÉ‚Äç‚ôÇÔ∏è Training SVM models (this may take a few minutes)...\")\n",
    "svm_grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Best SVM model\n",
    "best_svm = svm_grid_search.best_estimator_\n",
    "\n",
    "print(f\"‚úÖ Best SVM Parameters Found:\")\n",
    "for param, value in svm_grid_search.best_params_.items():\n",
    "    print(f\"   ‚Ä¢ {param}: {value}\")\n",
    "\n",
    "print(f\"üìä Best Cross-Validation Score: {svm_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Train and evaluate the best SVM model\n",
    "y_pred_svm = best_svm.predict(X_test_selected)\n",
    "y_pred_proba_svm = best_svm.predict_proba(X_test_selected)\n",
    "\n",
    "# Calculate metrics\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "svm_f1 = f1_score(y_test, y_pred_svm, average='macro')\n",
    "svm_cv_score = cross_val_score(best_svm, X_train_selected, y_train, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "print(f\"\\nüìà Support Vector Machine Results:\")\n",
    "print(f\"   ‚Ä¢ Test Accuracy: {svm_accuracy:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score (Macro): {svm_f1:.4f}\")\n",
    "print(f\"   ‚Ä¢ Cross-Validation Score: {svm_cv_score:.4f}\")\n",
    "\n",
    "print(f\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm,\n",
    "                          target_names=['Low', 'Medium-Low', 'Medium-High', 'High']))\n",
    "\n",
    "print(f\"\\nüîß SVM Model Details:\")\n",
    "print(f\"   ‚Ä¢ Kernel: {best_svm.kernel}\")\n",
    "print(f\"   ‚Ä¢ Number of Support Vectors: {best_svm.n_support_}\")\n",
    "print(f\"   ‚Ä¢ Support Vector Ratio: {best_svm.n_support_.sum() / len(X_train_selected):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compile results\n",
    "models_results = {\n",
    "    'Model': ['Bayesian Logistic Regression', 'Support Vector Machine'],\n",
    "    'Test_Accuracy': [lr_accuracy, svm_accuracy],\n",
    "    'F1_Score_Macro': [lr_f1, svm_f1],\n",
    "    'CV_Score': [lr_cv_score, svm_cv_score],\n",
    "    'Best_Parameters': [\n",
    "        str(log_grid_search.best_params_),\n",
    "        str(svm_grid_search.best_params_)\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(models_results)\n",
    "\n",
    "print(f\"üèÜ Performance Comparison:\")\n",
    "print(comparison_df[['Model', 'Test_Accuracy', 'F1_Score_Macro', 'CV_Score']].to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = comparison_df['F1_Score_Macro'].idxmax()\n",
    "best_model_name = comparison_df.iloc[best_model_idx]['Model']\n",
    "best_f1_score = comparison_df.iloc[best_model_idx]['F1_Score_Macro']\n",
    "\n",
    "print(f\"\\nü•á Best Performing Model: {best_model_name}\")\n",
    "print(f\"   üìà F1-Score: {best_f1_score:.4f}\")\n",
    "\n",
    "# Save results\n",
    "safe_save_file(comparison_df, 'data/processed/classification_model_comparison.csv', index=False)\n",
    "\n",
    "# Create detailed results dictionary for plotting\n",
    "results_dict = {\n",
    "    'Bayesian_LR': {\n",
    "        'model': best_bayesian_lr,\n",
    "        'predictions': y_pred_lr,\n",
    "        'probabilities': y_pred_proba_lr,\n",
    "        'accuracy': lr_accuracy,\n",
    "        'f1': lr_f1\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': best_svm,\n",
    "        'predictions': y_pred_svm,\n",
    "        'probabilities': y_pred_proba_svm,\n",
    "        'accuracy': svm_accuracy,\n",
    "        'f1': svm_f1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: data/processed/classification_model_comparison.csv\")\n",
    "print(f\"üìÅ Models trained and ready for visualization!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
